# -*- coding: utf-8 -*-
"""Sentiment Analysis with Hugging Face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rg480zclVXokQ5zsMGKLV92oyW4-eiDt

### **Data Ingestion:**
"""

!pip install -q datasets

from datasets import load_dataset

emotions = load_dataset("dair-ai/emotion")

"""### **Exploratory Data Analysis (EDA):**"""

emotions

train_ds = emotions["train"]
train_ds

len(train_ds)

train_ds[1]

train_ds.column_names

train_ds.features

train_ds[:5]

train_ds["text"][:5]

"""### **Converting Dataset to Pandas DataFrame:**"""

import pandas as pd

emotions.set_format(type="pandas")
df = emotions["train"][:]
df.head()

def label_int2str(row):
    return emotions["train"].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str)
df.head()

"""### **Data Visualization:**"""

import matplotlib.pyplot as plt

df["label_name"].value_counts(ascending=True).plot.barh()

plt.title("Class Distribution")
plt.show()

df["Words Per Tweet"] = df["text"].str.split().apply(len)

df.boxplot("Words Per Tweet", by="label_name", grid=False, showfliers=False, color="black")

plt.suptitle("")
plt.xlabel("")
plt.show()

emotions.reset_format()

"""### **Data Preprocessing:**

**1.** Character-Level Tokenization:
"""

text = "It is fun to work with NLP using HuggingFace."
tokenized_text = list(text)
print(tokenized_text)

token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}
print(token2idx)

input_ids = [token2idx[token] for token in tokenized_text]
print(input_ids)

"""**2.** Understanding One-Hot Encoding:"""

df = pd.DataFrame({
    "name": ["can", "efe", "ada"],
    "label": [0, 1, 2]
})
df

pd.get_dummies(df, dtype=int)

"""**3.** One-Hot Encoding in PyTorch:"""

import torch
import torch.nn.functional as F

input_ids = torch.tensor(input_ids)
one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))
one_hot_encodings.shape

print(f"Token: {tokenized_text[0]}")
print(f"Tensor Index: {input_ids[0]}")
print(f"One-Hot Encoding: {one_hot_encodings[0]}")

"""**4.** Word-Level Tokenization:"""

tokenized_text = text.split()
print(tokenized_text)

"""**5.** Subword-Level Tokenization:"""

from transformers import AutoTokenizer

model_ckpt = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

"""**6.** Custom Tokenization:"""

from transformers import DistilBertTokenizer

distbert_tokenize = DistilBertTokenizer.from_pretrained(model_ckpt)

"""**7.** Working with Tokenizers:"""

encoded_text = tokenizer(text)
print(encoded_text)

tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)
print(tokens)

tokenizer.convert_tokens_to_string(tokens)

"""**8.** Tokenizer Attributes:"""

tokenizer.vocab_size

tokenizer.model_max_length

"""**9.** Batch Tokenization:"""

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True)

print(tokenize(emotions["train"][:2]))

emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

"""**10.** Sequence Padding:"""

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

emotions_encoded["train"].column_names

"""### **Model Training:**"""

from transformers import AutoModelForSequenceClassification

num_labels = 6
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)

"""**1.** Evaluate:"""

!pip install -q evaluate

import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

"""**2.** Logging to Hugging Face Hub:"""

from huggingface_hub import notebook_login

notebook_login()

"""**3.** Configuring Training Arguments:"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="distilbert-emotion",
    num_train_epochs=2,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
    report_to="none"
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=emotions_encoded["train"],
    eval_dataset=emotions_encoded["validation"],
    tokenizer=tokenizer,
)

trainer.train()

"""### **Model Evaluation:**

**1.** Evaluating on the Validation Dataset:
"""

preds_output = trainer.predict(emotions_encoded["validation"])

preds_output.metrics

"""**2.** Confusion Matrix:"""

from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

y_preds = np.argmax(preds_output.predictions, axis=1)

def plot_confusion_matrix(y_preds, y_true, labels):
    cm = confusion_matrix(y_true, y_preds, normalize="true")
    fig, ax = plt.subplots(figsize=(6,6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap="Blues", values_format=".2f", ax=ax, colorbar=False)
    plt.title("Normalized Confusion Matrix")
    plt.show()

y_valid = np.array(emotions_encoded["validation"]["label"])
labels = emotions["train"].features["label"].names

plot_confusion_matrix(y_preds, y_valid, labels)

trainer.push_to_hub(commit_message="Model Training Successfully Completed!")

"""### **Model Inference:**"""

from transformers import pipeline

model_id = "kevin17daniel/distilbert-emotion"
classifier = pipeline("text-classification", model=model_id)

custom_text = "Pope Francis is showing a ‘good response’ to treatment, Vatican says."

preds = classifier(custom_text, return_all_scores=True)
preds_df = pd.DataFrame(preds[0])
plt.bar(labels, 100 * preds_df["score"])
plt.ylabel("Class Probability (%)")
plt.show()

custom_text = "NASA’s Artemis I mission has successfully launched, marking a significant milestone in lunar exploration."

preds = classifier(custom_text, return_all_scores=True)
preds_df = pd.DataFrame(preds[0])
plt.bar(labels, 100 * preds_df["score"])
plt.ylabel("Class Probability (%)")
plt.show()